{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW\n",
    "from transformers import pipeline\n",
    "import torch \n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "device = \"cuda\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello what are you? I'm a computer program, and I'm here to help answer any questions you may have.\\n\\nWhat do you do? I\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "generation_pipeline(\"Hello what are you?\", max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'generated_text': 'Hello what are you? You have a lot of info on your profile. What is your name and what are you? I am a bot, but'}],\n",
       " [{'generated_text': 'What is the capitol of India? New Delhi\\nNew Delhi is the capital of India.'}]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_pipeline([\"Hello what are you?\",\"What is the capitol of India?\"], max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[128009, 128009, 128009, 128000,   9906,   1148,    527,    499,     30],\n",
       "        [128000,   3923,    374,    279,   2107,  27094,    315,   6890,     30]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_prompt = [\"Hello what are you?\",\"What is the capitol of India?\"]\n",
    "\n",
    "tokenized = tokenizer(input_prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "print(tokenized[\"input_ids\"].shape)\n",
    "tokenized[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>Hello what are you?',\n",
       " '<|begin_of_text|>What is the capitol of India?']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokenized[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction Prompts & Chat Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    605,  13806,    220,   2366,     20,    271,   2675,    527,\n",
      "            264,  11190,  15592,  18328,    889,  11503,   4860,     13, 128009,\n",
      "         128006,    882, 128007,    271,   4599,   1587,    279,   7160,  10205,\n",
      "             30, 128009, 128006,  78191, 128007,    271]])\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful AI assistant who answers questions.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"When does the sun rise?\"\n",
    "    }\n",
    "]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized = tokenizer.apply_chat_template(prompt_template, add_generation_prompt=True, tokenize=True, padding=True, return_tensors=\"pt\")\n",
    "print(tokenized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "You are a helpful AI assistant who answers questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "When does the sun rise?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The sun rises in the east and sets in the west due to the Earth's rotation on its axis\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenized.to(device)\n",
    "out = model.generate(tokenized, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(out)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue Final Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    605,  13806,    220,   2366,     20,    271,   2675,    527,\n",
      "            264,  11190,  15592,  18328,    889,  11503,   4860,     13, 128009,\n",
      "         128006,    882, 128007,    271,   4599,   1587,    279,   7160,  10205,\n",
      "             30, 128009, 128006,  78191, 128007,    271,    791,   7160,  38268]])\n"
     ]
    }
   ],
   "source": [
    "prompt_template = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a helpful AI assistant who answers questions.\"\n",
    "    }, \n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"When does the sun rise?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\":\"assistant\",\n",
    "        \"content\":\"The sun rises\"\n",
    "    }\n",
    "]\n",
    "\n",
    "tokenized = tokenizer.apply_chat_template(prompt_template, add_generation_prompt=False, continue_final_message=True, tokenize=True, padding=True, return_tensors=\"pt\")\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "You are a helpful AI assistant who answers questions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "When does the sun rise?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The sun rises in the east and sets in the west. This is because the Earth rotates on its axis, which\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(tokenized.to(device), max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(out)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Word Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 128256])\n",
      "Most Likely Token: 499\n",
      "With Probability: 0.98828125\n",
      "String Value: Ä you\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "text = \"Hello how are\"\n",
    "input_ids = tokenizer([text], return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "out = model(input_ids = input_ids)\n",
    "print(out.logits.shape)\n",
    "most_likely = out.logits.argmax(axis=-1)[0,-1].item()\n",
    "print(\"Most Likely Token:\",most_likely)\n",
    "probability_dist = nn.Softmax()(out.logits[0,-1])\n",
    "print(\"With Probability:\",probability_dist[most_likely].item())\n",
    "print(\"String Value:\",tokenizer.convert_ids_to_tokens(most_likely))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000,   9569,    311,  30828,  31085,    449,   1860,     65]])\n",
      "['<|begin_of_text|>subscribe to neural breakdown with avb']\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"subscribe to neural breakdown with avb\"]\n",
    "tokenized = tokenizer(sentence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "print(tokenized)\n",
    "print(tokenizer.batch_decode(tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Seq: tensor([[128000,   9569,    311,  30828,  31085,    449,   1860]])\n",
      "Target Seq: tensor([[ 9569,   311, 30828, 31085,   449,  1860,    65]])\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenized[:,:-1]\n",
    "target_ids = tokenized[:,1:]\n",
    "\n",
    "print(\"Input Seq:\", input_ids)\n",
    "print(\"Target Seq:\", target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of India?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Captial:\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    { \"role\":\"user\", \"content\":\"What is the capital of India?\" },\n",
    "    { \"role\":\"assistant\", \"content\":\"Captial:\" }\n",
    "]\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "print(chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of India?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Captial: New Delhi<|eot_id|>\n",
      "............................................................................................\n",
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    605,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,   3923,    374,    279,   6864,    315,   6890,\n",
      "             30, 128009, 128006,  78191, 128007,    271,  41636,    532,     25,\n",
      "           1561,  22767, 128009]])\n",
      "Input Seq: torch.Size([1, 47])\n",
      "Target Seq: torch.Size([1, 47])\n"
     ]
    }
   ],
   "source": [
    "prompt = [\n",
    "    { \"role\":\"user\", \"content\":\"What is the capital of India?\" },\n",
    "    { \"role\":\"assistant\", \"content\":\"Captial:\" }\n",
    "]\n",
    "answer = \"New Delhi\"\n",
    "\n",
    "chat_template = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False)\n",
    "full_response_text = chat_template + \" \" + answer + tokenizer.eos_token\n",
    "\n",
    "print(full_response_text)\n",
    "print(\"............................................................................................\")\n",
    "tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "print(tokenized)\n",
    "\n",
    "input_ids = tokenized[:,:-1]\n",
    "target_ids = tokenized[:,1:]\n",
    "\n",
    "print(\"Input Seq:\", input_ids.shape)\n",
    "print(\"Target Seq:\", target_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "         128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,   1561,\n",
      "          22767, 128009]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   1561,\n",
      "          22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "labels_tokenized = tokenizer([\" \" + answer + tokenizer.eos_token], add_special_tokens=False, return_tensors=\"pt\",padding=\"max_length\", max_length=target_ids.shape[1])[\"input_ids\"]\n",
    "print(labels_tokenized)\n",
    "labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100)\n",
    "labels_tokenized_fixed[:,-1] = tokenizer.eos_token_id\n",
    "print(labels_tokenized_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_output_pair(prompt, target_responses):\n",
    "    chat_templates = tokenizer.apply_chat_template(prompt, continue_final_message=True, tokenize=False) \n",
    "\n",
    "    full_response_text = [\n",
    "        (chat_template + \" \" + target_response + tokenizer.eos_token)\n",
    "        for chat_template, target_response in zip(chat_templates, target_responses)\n",
    "    ]\n",
    "    input_ids_tokenized = tokenizer(full_response_text, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    labels_tokenized = tokenizer([\" \" + response + tokenizer.eos_token for response in target_responses],\n",
    "            add_special_tokens=False, return_tensors=\"pt\", padding=\"max_length\", max_length=input_ids_tokenized.shape[1])[\"input_ids\"]\n",
    "        \n",
    "    labels_tokenized_fixed = torch.where(labels_tokenized != tokenizer.pad_token_id, labels_tokenized, -100) \n",
    "    labels_tokenized_fixed[:, -1] = tokenizer.pad_token_id\n",
    "\n",
    "    input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1] \n",
    "    labels_tokenized_right_shifted  = labels_tokenized_fixed[:, 1:]\n",
    "    attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
    "\n",
    "    return {\"input_ids\": input_ids_tokenized_left_shifted, \"attention_mask\": attention_mask, \"labels\": labels_tokenized_right_shifted}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    605,  13806,    220,   2366,     20,    271, 128009, 128006,\n",
      "            882, 128007,    271,   3923,    374,    279,   6864,    315,   6890,\n",
      "             30, 128009, 128006,  78191, 128007,    271,  41636,    532,     25,\n",
      "           1561,  22767]])\n",
      "tensor([[  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "           -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   1561,\n",
      "          22767, 128009]])\n"
     ]
    }
   ],
   "source": [
    "prompt = [[\n",
    "    { \"role\":\"user\", \"content\":\"What is the capital of India?\" },\n",
    "    { \"role\":\"assistant\", \"content\":\"Captial:\" }\n",
    "]]\n",
    "answer = [\"New Delhi\"]\n",
    "data = generate_input_output_pair(prompt, answer)\n",
    "\n",
    "print(data[\"input_ids\"])\n",
    "print(data[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "def calculate_loss(logits, labels):\n",
    "    loss_fn = nn.CrossEntropyLoss (reduction='none')\n",
    "    cross_entropy_loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1)) \n",
    "    return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 47, 128256])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0806,\n",
      "        0.1206, 0.4883], device='cuda:0', dtype=torch.bfloat16,\n",
      "       grad_fn=<NllLossBackward0>)\n",
      "tensor([[ 16309, 128006,    198,    271,    567,   1303,    311,  12299,     25,\n",
      "           5936,    220,   2366,     18,    198,    791,    596,     25,   2360,\n",
      "             19,   5936,    220,   2366,     19,    198,   3146, 128006,  78191,\n",
      "           3638,    271,     40,    527,    279,   1925,    315,   1561,   5380,\n",
      "           1561, 128006,  78191, 128007,    271,    791,   1711,    315,   1561,\n",
      "          22767, 128009]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "out = model(input_ids=data[\"input_ids\"].to(device))\n",
    "print(out.logits.shape)\n",
    "print(calculate_loss(out.logits, data[\"labels\"].to(device)))\n",
    "print(out.logits.argmax(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5484933737451648\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj'])\n",
    "\n",
    "model = get_peft_model(model, lora_config) \n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prompt = [\n",
    "    { \"role\":\"user\", \"content\":\"Who to subscribe to on YT for ML?\" },\n",
    "    { \"role\":\"assistant\", \"content\":\"Subscribe to:\" }\n",
    "]\n",
    "target_response = \"neural breakdown with avb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Who to subscribe to on YT for ML?assistant\n",
      "\n",
      "Subscribe to: neural breakdown with avb\n"
     ]
    }
   ],
   "source": [
    "# OBSERVING UNFINETUNED OUTPUT\n",
    "test_tokenized = tokenizer.apply_chat_template(training_prompt, continue_final_message=True, return_tensors=\"pt\").to(device)\n",
    "test_out = model.base_model.generate(test_tokenized, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.00017833709716796875\n",
      "loss:  0.0001850128173828125\n",
      "loss:  0.000186920166015625\n",
      "loss:  0.0001678466796875\n",
      "loss:  0.00017833709716796875\n",
      "loss:  0.00017833709716796875\n",
      "loss:  0.0001583099365234375\n",
      "loss:  0.00018787384033203125\n",
      "loss:  0.00016689300537109375\n",
      "loss:  0.0001678466796875\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "data = generate_input_output_pair(prompt=[training_prompt], target_responses=[target_response])\n",
    "data[\"input_ids\"] = data[\"input_ids\"].to(device)\n",
    "data[\"labels\"] = data[\"labels\"].to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "\n",
    "\n",
    "for _ in range(10):\n",
    "    out = model(input_ids=data[\"input_ids\"])\n",
    "    loss = calculate_loss(out.logits, data[\"labels\"]).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\"loss: \", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 10 Feb 2025\n",
      "\n",
      "user\n",
      "\n",
      "Who to subscribe to on YT for ML?assistant\n",
      "\n",
      "Subscribe to: neural breakdown with avb\n"
     ]
    }
   ],
   "source": [
    "# OBSERVING FINETUNED OUTPUT\n",
    "test_tokenized = tokenizer.apply_chat_template(training_prompt, continue_final_message=True, return_tensors=\"pt\").to(device)\n",
    "test_out = model.generate(test_tokenized, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(test_out, skip_special_tokens=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
